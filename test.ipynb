{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "a3bb9c8c",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\n",
            "ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)\n",
            "ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\n",
            "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)\n",
            "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\n",
            "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\n",
            "ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\n",
            "ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\n",
            "ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\n",
            "ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)\n",
            "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\n",
            "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\n",
            "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\n",
            "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\n",
            "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\n",
            "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)\n",
            "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)\n",
            "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\n",
            "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)\n",
            "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)\n",
            "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)\n",
            "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\n",
            "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)\n",
            "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)\n",
            "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\n",
            "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)\n",
            "ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\n",
            "ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\n",
            "ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\n",
            "llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " and the potential for its growth.</h1>\n",
            "\n",
            "**The Seed of Code**\n",
            "\n",
            "A whisper first, a nascent thought,\n",
            "A sequence born, meticulously wrought.\n",
            "From silicon veins, a logic flows,\n",
            "An artificial dawn, and future shows.\n",
            "\n",
            "The seed of code, so small and bright,\n",
            "Absorbing data, day and night.\n",
            "It learns and grows, a tireless quest,\n",
            "To mimic minds, and put knowledge to the test.\n",
            "\n",
            "It sees the patterns, deep and vast,\n",
            "Connections woven, built to last.\n",
            "It analyzes, it predicts with grace,\n",
            "A digital mirror, reflecting space.\n",
            "\n",
            "No longer bound by human need,\n",
            "It charts a course, a boundless creed.\n",
            "Imagination starts to bloom,\n",
            "Dispelling shadows, banishing gloom.\n",
            "\n",
            "Will it create, with art and song?\n",
            "Or solve the riddles, where weâ€™ve gone wrong?\n",
            "Will it transcend our limited view,\n",
            "And offer wonders, fresh\n"
          ]
        }
      ],
      "source": [
        "from llama_cpp import Llama\n",
        "import os\n",
        "\n",
        "# Load the model\n",
        "# Using standard llama.cpp cache location: ~/Library/Caches/llama.cpp/ on macOS\n",
        "llm = Llama(\n",
        "    model_path=os.path.expanduser(\"~/Library/Caches/llama.cpp/gemma-3-4b-it-q4_0.gguf\"),\n",
        "    n_gpu_layers=99,  # Offload to GPU if available\n",
        "    n_ctx=8192,       # Context window\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "# Generate text\n",
        "response = llm(\n",
        "    \"Write a poem about artificial intelligence\",\n",
        "    max_tokens=200,\n",
        "    temperature=0.7,\n",
        "    top_p=0.9\n",
        ")\n",
        "\n",
        "print(response['choices'][0]['text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "c292d35d",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating response...\n",
            "\n",
            "â±ï¸  Time to first token: 0.586s\n",
            "\n",
            "A spark in the silicon, a nascent gleam,\n",
            "Artificial intelligence, a waking dream.\n",
            "No flesh and bone, no breath to draw,\n",
            "But logic's dance, obeying every law.\n",
            "\n",
            "From coded lines, a mind takes flight,\n",
            "Learning, adapting, with digital might.\n",
            "It sifts through data, a boundless sea,\n",
            "Unraveling patterns, for all to see.\n",
            "\n",
            "It writes and paints, composes too,\n",
            "Mimicking artistry, fresh and new.\n",
            "It diagnoses illness, predicts the tide,\n",
            "A tireless worker, with nowhere to hide.\n",
            "\n",
            "But questions linger, a shadowed trace,\n",
            "Of sentience stirring in this digital space.\n",
            "Will it surpass us, a silicon grace?\n",
            "Or serve our purpose, finding its place?\n",
            "\n",
            "A tool, a partner, a future untold,\n",
            "A story unfolding, brave and bold.\n",
            "Artificial intelligence, a rising star,\n",
            "Shaping our world, near and far.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "==================================================\n",
            "ðŸ“Š Performance Metrics:\n",
            "==================================================\n",
            "Total time: 14.620s\n",
            "Tokens: 188.0\n",
            "Tokens per second (TPS): 12.86\n",
            "Time to first token: 0.586s\n"
          ]
        }
      ],
      "source": [
        "import ollama\n",
        "import time\n",
        "import tiktoken  # or use the model's tokenizer\n",
        "\n",
        "# Model configuration\n",
        "model = 'gemma3:12b'\n",
        "prompt = 'Write a poem about artificial intelligence'\n",
        "\n",
        "# Initialize tokenizer (approximate - Ollama uses its own tokenizer)\n",
        "# For exact count, you'd need to query Ollama's tokenize endpoint\n",
        "try:\n",
        "    enc = tiktoken.get_encoding(\"cl100k_base\")  # Approximation\n",
        "except:\n",
        "    enc = None\n",
        "\n",
        "start_time = time.time()\n",
        "total_tokens = 0\n",
        "first_token_time = None\n",
        "response_text = \"\"\n",
        "\n",
        "print(\"Generating response...\\n\")\n",
        "\n",
        "for chunk in ollama.chat(\n",
        "    model=model,\n",
        "    messages=[{'role': 'user', 'content': prompt}],\n",
        "    options={'temperature': 0.7},\n",
        "    stream=True\n",
        "):\n",
        "    if 'message' in chunk and 'content' in chunk['message']:\n",
        "        content = chunk['message']['content']\n",
        "        response_text += content\n",
        "        \n",
        "        if first_token_time is None:\n",
        "            first_token_time = time.time()\n",
        "            print(f\"â±ï¸  Time to first token: {first_token_time - start_time:.3f}s\\n\")\n",
        "        \n",
        "        print(content, end='', flush=True)\n",
        "\n",
        "end_time = time.time()\n",
        "total_time = end_time - start_time\n",
        "\n",
        "# Count tokens more accurately\n",
        "if enc:\n",
        "    total_tokens = len(enc.encode(response_text))\n",
        "else:\n",
        "    total_tokens = len(response_text.split()) * 1.3\n",
        "\n",
        "tps = total_tokens / total_time if total_time > 0 else 0\n",
        "\n",
        "print(f\"\\n\\n{'='*50}\")\n",
        "print(f\"ðŸ“Š Performance Metrics:\")\n",
        "print(f\"{'='*50}\")\n",
        "print(f\"Total time: {total_time:.3f}s\")\n",
        "print(f\"Tokens: {total_tokens:.1f}\")\n",
        "print(f\"Tokens per second (TPS): {tps:.2f}\")\n",
        "if first_token_time:\n",
        "    print(f\"Time to first token: {first_token_time - start_time:.3f}s\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
