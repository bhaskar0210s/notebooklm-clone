{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Test Retrieval Graph\n",
        "\n",
        "This notebook tests the retrieval graph by connecting to the LangGraph server.\n",
        "\n",
        "**Prerequisites:**\n",
        "1. Start the LangGraph dev server: `cd backend && npm run dev`\n",
        "2. Make sure the server is running at `http://localhost:2024`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages (run once)\n",
        "# %pip install langgraph-sdk\n",
        "# Ollama is running locally for this test."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langgraph_sdk import get_client\n",
        "\n",
        "# Connect to the local LangGraph server\n",
        "# Make sure you've started it with: cd backend && npm run dev\n",
        "client = get_client(url=\"http://localhost:2024\")\n",
        "\n",
        "print(\"‚úÖ Connected to LangGraph server\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a new thread (conversation session)\n",
        "thread = await client.threads.create()\n",
        "print(f\"‚úÖ Created thread: {thread['thread_id']}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Test 1: Simple Query (Non-streaming)\n",
        "\n",
        "# Test with a simple query\n",
        "test_query = \"What is the capital of France?\"\n",
        "\n",
        "print(f\"Query: {test_query}\\n\")\n",
        "\n",
        "result = await client.runs.create(\n",
        "    thread_id=thread['thread_id'],\n",
        "    assistant_id=\"retrieval_graph\",\n",
        "    input={\"query\": test_query},\n",
        "    \n",
        ")\n",
        "\n",
        "print(\"‚úÖ Graph execution completed!\")\n",
        "print(f\"\\nResult: {result}\")\n",
        "print(f\"\\nStatus: {result.get('status', 'unknown')}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get the threads\n",
        "threads = await client.threads.search()\n",
        "print(threads)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get the thread state which contains messages\n",
        "thread_state = await client.threads.get(thread_id=thread['thread_id'])\n",
        "\n",
        "# Access messages from the thread's state\n",
        "messages = thread_state.get('values', {}).get('messages', [])\n",
        "\n",
        "print(\"Messages in thread:\")\n",
        "for msg in messages:\n",
        "    msg_type = msg.get('type') if isinstance(msg, dict) else getattr(msg, 'type', 'unknown')\n",
        "    msg_content = msg.get('content') if isinstance(msg, dict) else getattr(msg, 'content', '')\n",
        "    print(f\"\\n- {msg_type}: {msg_content}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import asyncio\n",
        "\n",
        "# Test with a simple query\n",
        "test_query = \"Add 600 + 3000\"\n",
        "\n",
        "print(f\"Query: {test_query}\\n\")\n",
        "\n",
        "result = await client.runs.create(\n",
        "    thread_id=thread['thread_id'],\n",
        "    assistant_id=\"retrieval_graph\",\n",
        "    input={\"query\": test_query},\n",
        "    # config={\n",
        "    #     \"configurable\": {\n",
        "    #         \"queryModel\": \"ollama/llama3.2:1b\"\n",
        "    #     }\n",
        "    # }\n",
        ")\n",
        "\n",
        "# cancel the run after 100ms\n",
        "run_id = result.get('run_id') if isinstance(result, dict) else getattr(result, 'run_id', None)\n",
        "thread_id = thread['thread_id']\n",
        "if run_id:\n",
        "    await asyncio.sleep(0.1)\n",
        "    \n",
        "    target_run_id = run_id or globals().get('current_run_id')\n",
        "    if not target_run_id:\n",
        "        print(\"‚ùå No run ID found.\")\n",
        "    else:\n",
        "        # Cancel the run\n",
        "        await client.runs.cancel(thread_id=thread_id, run_id=target_run_id)\n",
        "        print(f\"‚úÖ Canceled\")\n",
        "\n",
        "\n",
        "# Get the thread state which contains messages\n",
        "thread_state = await client.threads.get(thread_id=thread['thread_id'])\n",
        "\n",
        "# Access messages from the thread's state\n",
        "messages = thread_state.get('values', {}).get('messages', [])\n",
        "\n",
        "if messages:\n",
        "    # Get the latest message (last in the list)\n",
        "    latest = messages[-1]\n",
        "    latest_content = latest.get('content') if isinstance(latest, dict) else getattr(latest, 'content', None)\n",
        "    if latest_content:\n",
        "        print(f\"Response: {latest_content}\")\n",
        "print(thread_state.get('values', {}).get('messages', []))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get the thread state which contains messages\n",
        "thread_state = await client.threads.get(thread_id=thread['thread_id'])\n",
        "\n",
        "# Access messages from the thread's state\n",
        "messages = thread_state.get('values', {}).get('messages', [])\n",
        "\n",
        "if messages:\n",
        "    # Get the latest message (last in the list)\n",
        "    latest = messages[-1]\n",
        "    latest_content = latest.get('content') if isinstance(latest, dict) else getattr(latest, 'content', None)\n",
        "    if latest_content:\n",
        "        print(f\"Response: {latest_content}\")\n",
        "print(thread_state.get('values', {}).get('messages', []))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Test 1: Simple Query (Non-streaming)\n",
        "\n",
        "# Test with a simple query\n",
        "test_query = \"What is the capital of France?\"\n",
        "\n",
        "print(f\"Query: {test_query}\\n\")\n",
        "\n",
        "result = await client.runs.create(\n",
        "    thread_id=thread['thread_id'],\n",
        "    assistant_id=\"retrieval_graph\",\n",
        "    input={\"query\": test_query},\n",
        "    \n",
        ")\n",
        "\n",
        "print(\"‚úÖ Graph execution completed!\")\n",
        "print(f\"\\nResult: {result}\")\n",
        "print(f\"\\nStatus: {result.get('status', 'unknown')}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get the thread state which contains messages\n",
        "thread_state = await client.threads.get(thread_id=thread['thread_id'])\n",
        "\n",
        "# Access messages from the thread's state\n",
        "messages = thread_state.get('values', {}).get('messages', [])\n",
        "\n",
        "print(\"Messages in thread:\")\n",
        "for msg in messages:\n",
        "    msg_type = msg.get('type') if isinstance(msg, dict) else getattr(msg, 'type', 'unknown')\n",
        "    msg_content = msg.get('content') if isinstance(msg, dict) else getattr(msg, 'content', '')\n",
        "    print(f\"\\n- {msg_type}: {msg_content}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Test 2: Streaming Response\n",
        "\n",
        "# Test with streaming\n",
        "test_query_2 = \"Tell me a joke\"\n",
        "\n",
        "print(f\"Query: {test_query_2}\\n\")\n",
        "print(\"Streaming response:\")\n",
        "\n",
        "# Remove 'await' here - stream() returns an async generator directly\n",
        "stream = client.runs.stream(\n",
        "    thread_id=thread['thread_id'],\n",
        "    assistant_id=\"retrieval_graph\",\n",
        "    input={\"query\": test_query_2},\n",
        "    stream_mode=[\"messages\", \"updates\"],\n",
        "    config={\n",
        "        \"configurable\": {\n",
        "            \"queryModel\": \"ollama/llama3.2:1b\"\n",
        "        }\n",
        "    }\n",
        ")\n",
        "\n",
        "async for chunk in stream:\n",
        "    chunk_event = chunk.get('event') if isinstance(chunk, dict) else getattr(chunk, 'event', None)\n",
        "    if chunk_event == 'messages':\n",
        "        chunk_data = chunk.get('data') if isinstance(chunk, dict) else getattr(chunk, 'data', None)\n",
        "        if chunk_data:\n",
        "            for msg in chunk_data:\n",
        "                msg_content = msg.get('content') if isinstance(msg, dict) else getattr(msg, 'content', None)\n",
        "                if msg_content:\n",
        "                    print(f\"üì® {msg_content}\")\n",
        "    elif chunk_event == 'updates':\n",
        "        print(f\"üîÑ Update: {chunk}\")\n",
        "    else:\n",
        "        print(f\"üì¶ Chunk: {chunk}\")\n",
        "\n",
        "print(\"\\n‚úÖ Streaming completed!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Test 3: Multiple Queries in Same Thread\n",
        "\n",
        "thread = await client.threads.create()\n",
        "# Test multiple queries in the same thread (conversation history)\n",
        "queries = [\n",
        "    \"What is 2 + 2?\",\n",
        "    \"What about 3 + 3?\",\n",
        "    \"Can you add those two answers together?\"\n",
        "]\n",
        "\n",
        "import asyncio\n",
        "\n",
        "for i, query in enumerate(queries, 1):\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(f\"Query {i}: {query}\")\n",
        "    print('='*50)\n",
        "    \n",
        "    result = await client.runs.create(\n",
        "        thread_id=thread['thread_id'],\n",
        "        assistant_id=\"retrieval_graph\",\n",
        "        input={\"query\": query},\n",
        "        config={\n",
        "            \"configurable\": {\n",
        "                # \"queryModel\": \"ollama/qwen3:4b\"\n",
        "                # \"queryModel\": \"ollama/llama3.2:1b\"\n",
        "            }\n",
        "        }\n",
        "    )\n",
        "    \n",
        "    # Wait for the run to complete by polling its status\n",
        "    run_id = result.get('run_id') if isinstance(result, dict) else getattr(result, 'run_id', None)\n",
        "    if run_id:\n",
        "        while True:\n",
        "            run_status = await client.runs.get(\n",
        "                thread_id=thread['thread_id'],\n",
        "                run_id=run_id\n",
        "            )\n",
        "            status = run_status.get('status') if isinstance(run_status, dict) else getattr(run_status, 'status', None)\n",
        "            if status in ['success', 'error', 'cancelled']:\n",
        "                break\n",
        "            await asyncio.sleep(0.5)  # Poll every 500ms\n",
        "    \n",
        "    # Get the thread state which contains messages\n",
        "    thread_state = await client.threads.get(thread_id=thread['thread_id'])\n",
        "    \n",
        "    # Access messages from the thread's state\n",
        "    messages = thread_state.get('values', {}).get('messages', [])\n",
        "    \n",
        "    if messages:\n",
        "        # Get the latest message (last in the list)\n",
        "        latest = messages[-1]\n",
        "        latest_content = latest.get('content') if isinstance(latest, dict) else getattr(latest, 'content', None)\n",
        "        if latest_content:\n",
        "            print(f\"Response: {latest_content}\")\n",
        "\n",
        "print(\"\\n‚úÖ All queries completed!\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
